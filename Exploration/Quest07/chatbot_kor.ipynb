{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca92e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2c26c",
   "metadata": {},
   "source": [
    "## 퀘스트 목표\n",
    "1. 한국어 전처리를 통해 학습 데이터셋을 구축하였다.\t                                  \n",
    "    - 공백과 특수문자 처리, 토크나이징, 병렬데이터 구축의 과정이 적절히 진행되었다.\n",
    "2. 트랜스포머 모델을 구현하여 한국어 챗봇 모델 학습을 정상적으로 진행하였다.\t        \n",
    "    - 구현한 트랜스포머 모델이 한국어 병렬 데이터 학습 시 안정적으로 수렴하였다.\n",
    "3. 한국어 입력문장에 대해 한국어로 답변하는 함수를 구현하였다.\t                       \n",
    "    - 한국어 입력문장에 맥락에 맞는 한국어로 답변을 리턴하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b198ce0",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 수집하기\n",
    "\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용. songys/Chatbot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22091c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드 \n",
    "import pandas as pd\n",
    "\n",
    "path_to_dataset = 'data'\n",
    "\n",
    "path_to_kor = os.path.join(path_to_dataset, 'ChatbotData.csv')\n",
    "\n",
    "\n",
    "chatbot_df = pd.read_csv(path_to_kor)\n",
    "chatbot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561fdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chatbot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5da4b8",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 전처리하기\n",
    "1. 대소문자 통일이 불필요(삭제)\n",
    "2. 전처리 시 데이터 한글만 남기도록 정규표현식 사용([^가-힣.,?!])\n",
    "3. 기존 파일에서 직접 업로드하는 형식 -> 데이터프레임 형식으로 데이터 핸들링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 양쪽 공백을 제거\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "    # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "    # student와 온점 사이에 거리를 만듭니다.\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # (가-힣, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "    sentence = re.sub(r\"[^가-힣.,?!]\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conversations(df):\n",
    "    inputs, outputs = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        # 전처리 함수를 질문(Q)과 답변(A)에 적용\n",
    "        inputs.append(preprocess_sentence(row['Q']))\n",
    "        outputs.append(preprocess_sentence(row['A']))\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a02e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장합니다.\n",
    "questions, answers = load_conversations(chatbot_df)\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 22번째 질문 샘플: 가스비 장난 아님\n",
      "전처리 후의 22번째 답변 샘플: 다음 달에는 더 절약해봐요 .\n"
     ]
    }
   ],
   "source": [
    "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[21]))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[21]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0822b",
   "metadata": {},
   "source": [
    "## Step 3. SubwordTextEncoder 사용하기\n",
    "- 한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. \n",
    "- 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8132]\n",
      "END_TOKEN의 번호 : [8133]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "# vocab_size 조절해보기\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400c8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8134\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776f280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5750, 613, 2490, 4155]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2360, 7484, 7, 6250, 98, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3266b895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이: 29\n",
      "평균 길이: 5.65\n",
      "95% 백분위수: 10.0\n",
      "97% 백분위수: 11.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 질문과 답변의 토큰 길이 구하기\n",
    "question_lengths = [len(tokenizer.encode(q)) for q in questions]\n",
    "answer_lengths = [len(tokenizer.encode(a)) for a in answers]\n",
    "\n",
    "# 전체 길이 리스트\n",
    "all_lengths = question_lengths + answer_lengths\n",
    "\n",
    "# 길이 분포 시각화\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(all_lengths, bins=50, alpha=0.7, color='skyblue', label='Lengths')\n",
    "plt.axvline(x=np.percentile(all_lengths, 95), color='b', linestyle='--', label='95th Percentile')\n",
    "plt.axvline(x=np.percentile(all_lengths, 97), color='r', linestyle='--', label='95th Percentile')\n",
    "\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sequence Length Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 통계 분석\n",
    "print(f\"최대 길이: {np.max(all_lengths)}\")\n",
    "print(f\"평균 길이: {np.mean(all_lengths):.2f}\")\n",
    "print(f\"95% 백분위수: {np.percentile(all_lengths, 95)}\")\n",
    "print(f\"97% 백분위수: {np.percentile(all_lengths, 97)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe7475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 11\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "        # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "\n",
    "    # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597088b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8134\n",
      "필터링 후의 질문 샘플 개수: 10131\n",
      "필터링 후의 답변 샘플 개수: 10131\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622284d",
   "metadata": {},
   "source": [
    "## Step 4. 모델 구성하기\n",
    "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9328a47f",
   "metadata": {},
   "source": [
    "### PositionalEncoding()\n",
    "문장 내 단어 위치 정보를 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # 각도 배열 생성\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sin과 cosine이 교차되도록 재배열\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5337a",
   "metadata": {},
   "source": [
    "### scaled_dot_product_attention()\n",
    "어텐션 점수 계산(Q,K,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bd53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax적용\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0feb1",
   "metadata": {},
   "source": [
    "### MultiHeadAttention()\n",
    "어탠션 병렬 수행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        query = self.query_dense(query)\n",
    "        key = self.query_dense(key)\n",
    "        value = self.query_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce291ee",
   "metadata": {},
   "source": [
    "### create_padding_mask()\n",
    "패딩된 부분 무시(0으로 된 토큰을 1로 표시. 계산 시 패딩 영향 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a793cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a282484",
   "metadata": {},
   "source": [
    "### create_look_ahead_mask()\n",
    "디코더가 미래 단어를 참고하지 않도록 마스킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc337d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd202b5b",
   "metadata": {},
   "source": [
    "### encoder_layer()\n",
    "단일 인코더 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1085fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deffb4f",
   "metadata": {},
   "source": [
    "### encoder()\n",
    "여러개 인코더층을 쌓아서 만든 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad609184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a67e5",
   "metadata": {},
   "source": [
    "### decoder_layer() \n",
    "단일 디코더 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "        })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "        })\n",
    "\n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1d32e",
   "metadata": {},
   "source": [
    "### decoder()\n",
    "여러 디코더 층을 쌓은 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9248b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78420163",
   "metadata": {},
   "source": [
    "### transformer()\n",
    "인코더와 디코더를 결합한 최종 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "    # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # 인코더\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    # 디코더\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6634fee9",
   "metadata": {},
   "source": [
    "### 모델 컴파일 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b460030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ae5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd05c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0daa6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba12396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    2873344     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3137536     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8134)   2090438     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,101,318\n",
      "Trainable params: 8,101,318\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f615ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드에러 1\n",
    "#  NameError: name 'split_heads' is not defined\n",
    "# Multihead에서 split_head에서 클래스 메서드호출인 아닌 일반 메서드 호출 방식으로 코드 구현함\n",
    "# self.split_head()가 아닌 split_heads()으로 짜서 수정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18159747",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83852ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "159/159 [==============================] - 10s 27ms/step - loss: 5.1926 - accuracy: 0.1300\n",
      "Epoch 2/10\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 4.2790 - accuracy: 0.1928\n",
      "Epoch 3/10\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 3.5701 - accuracy: 0.1948\n",
      "Epoch 4/10\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 3.2333 - accuracy: 0.2049\n",
      "Epoch 5/10\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 3.0222 - accuracy: 0.2168\n",
      "Epoch 6/10\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 2.8290 - accuracy: 0.2298\n",
      "Epoch 7/10\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 2.6235 - accuracy: 0.2487\n",
      "Epoch 8/10\n",
      "159/159 [==============================] - 4s 28ms/step - loss: 2.4083 - accuracy: 0.2718\n",
      "Epoch 9/10\n",
      "159/159 [==============================] - 4s 28ms/step - loss: 2.1757 - accuracy: 0.2973\n",
      "Epoch 10/10\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 1.9417 - accuracy: 0.3201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7b704c4c15b0>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532396f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence, model):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "    # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "    # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "        # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe7c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence, model):\n",
    "    # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence, model)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "            \n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "    \n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트용 문자출력 함수\n",
    "def test_model(model):\n",
    "    answer_a = sentence_generation(\"오늘 날씨 어때?\", model)\n",
    "    print(\" \")\n",
    "    answer_b = sentence_generation(\"나 너무 슬퍼\", model)\n",
    "    print(\" \")\n",
    "    answer_c = sentence_generation(\"배고파\", model)\n",
    "    print(\" \")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b039d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 저도 좋아해요 .\n",
      " \n",
      "입력 : 나 너무 슬퍼\n",
      "출력 : 한 잔 드세요 .\n",
      " \n",
      "입력 : 배고파\n",
      "출력 : 얼른 드세요 .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3bf1f7",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# EarlyStopping 콜백 정의\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='accuracy',          # 'accuracy' 모니터링\n",
    "    patience=3,                  # 3 epoch 동안 개선되지 않으면 학습 중단\n",
    "    restore_best_weights=True,   # 가장 좋은 가중치를 복원\n",
    "    verbose=1                    # 조기 종료 시 메시지 출력\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2700c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 1.7027 - accuracy: 0.3445\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 1.4687 - accuracy: 0.3698\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 1.2403 - accuracy: 0.3991\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 1.0233 - accuracy: 0.4306\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.8235 - accuracy: 0.4633\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.6466 - accuracy: 0.4936\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.4944 - accuracy: 0.5224\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.3698 - accuracy: 0.5471\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.2822 - accuracy: 0.5637\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.2201 - accuracy: 0.5761\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1831 - accuracy: 0.5822\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1593 - accuracy: 0.5862\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1471 - accuracy: 0.5877\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1411 - accuracy: 0.5882\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.1309 - accuracy: 0.5910\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1256 - accuracy: 0.5915\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1125 - accuracy: 0.5944\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0984 - accuracy: 0.5981\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0893 - accuracy: 0.6006\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0830 - accuracy: 0.6023\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0722 - accuracy: 0.6047\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0665 - accuracy: 0.6065\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0654 - accuracy: 0.6066\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0581 - accuracy: 0.6084\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0542 - accuracy: 0.6095\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0487 - accuracy: 0.6109\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0475 - accuracy: 0.6114\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0439 - accuracy: 0.6120\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0426 - accuracy: 0.6125\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0400 - accuracy: 0.6130\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0353 - accuracy: 0.6144\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0349 - accuracy: 0.6147\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0336 - accuracy: 0.6143\n",
      "Epoch 34/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0297 - accuracy: 0.6158\n",
      "Epoch 35/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0300 - accuracy: 0.6157\n",
      "Epoch 36/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0294 - accuracy: 0.6159\n",
      "Epoch 37/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0281 - accuracy: 0.6162\n",
      "Epoch 38/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0260 - accuracy: 0.6171\n",
      "Epoch 39/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0254 - accuracy: 0.6170\n",
      "Epoch 40/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0240 - accuracy: 0.6173\n",
      "Epoch 41/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0227 - accuracy: 0.6175\n",
      "Epoch 42/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0217 - accuracy: 0.6175\n",
      "Epoch 43/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0233 - accuracy: 0.6177\n",
      "Epoch 44/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0215 - accuracy: 0.6179\n",
      "Epoch 45/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0195 - accuracy: 0.6182\n",
      "Epoch 46/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0197 - accuracy: 0.6184\n",
      "Epoch 47/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0194 - accuracy: 0.6181\n",
      "Epoch 48/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0193 - accuracy: 0.6183\n",
      "Epoch 49/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0173 - accuracy: 0.6187\n",
      "Epoch 50/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0183 - accuracy: 0.6187\n",
      "Epoch 51/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0163 - accuracy: 0.6189\n",
      "Epoch 52/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0166 - accuracy: 0.6191\n",
      "Epoch 53/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0148 - accuracy: 0.6192\n",
      "Epoch 54/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0154 - accuracy: 0.6192\n",
      "Epoch 55/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0151 - accuracy: 0.6192\n",
      "Epoch 56/100\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0161 - accuracy: 0.6191\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00056: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7b704fb22400>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1, callbacks=[early_stopping] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe11f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 날씨 어플에 물어보세요 .\n",
      " \n",
      "입력 : 나 너무 슬퍼\n",
      "출력 : 이제 그만 아파하면서 해보세요 .\n",
      " \n",
      "입력 : 배고파\n",
      "출력 : 뭐 좀 챙겨드세요 .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3fdb0f",
   "metadata": {},
   "source": [
    "## 추가. 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fdeb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 튜닝을 위한 모델 선언 및 컴파일 함수\n",
    "def train_model(parameter_dic):\n",
    "    BATCH_SIZE = parameter_dic['BATCH_SIZE']\n",
    "    BUFFER_SIZE = parameter_dic['BUFFER_SIZE']\n",
    "\n",
    "    # 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "    # 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'inputs': questions,\n",
    "            'dec_inputs': answers[:, :-1]\n",
    "        },\n",
    "        {\n",
    "            'outputs': answers[:, 1:]\n",
    "        },\n",
    "    ))\n",
    "\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # 하이퍼파라미터\n",
    "    NUM_LAYERS = parameter_dic['NUM_LAYERS'] # 인코더와 디코더의 층의 개수\n",
    "    D_MODEL = parameter_dic['D_MODEL'] # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "    NUM_HEADS = parameter_dic['NUM_HEADS'] # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "    UNITS = parameter_dic['UNITS'] # 피드 포워드 신경망의 은닉층의 크기\n",
    "    DROPOUT = parameter_dic['DROPOUT'] # 드롭아웃의 비율\n",
    "\n",
    "    model = transformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        units=UNITS,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT)\n",
    "    \n",
    "    learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "    \n",
    "    EPOCHS = parameter_dic['EPOCHS']\n",
    "    model.fit(dataset, epochs=EPOCHS, verbose=1, callbacks=[early_stopping] )\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dic={\n",
    "    'BATCH_SIZE' : 32, # 배치 사이즈\n",
    "    'BUFFER_SIZE' : 20000,# 버퍼 사이즈\n",
    "    'NUM_LAYERS' : 2, # 인코더와 디코더의 층의 개수\n",
    "    'D_MODEL' : 256, # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "    'NUM_HEADS' : 8, # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "    'UNITS' : 512, # 피드 포워드 신경망의 은닉층의 크기\n",
    "    'DROPOUT' : 0.1, # 드롭아웃의 비율\n",
    "    'EPOCHS' : 200 # 에포크\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "base_params = {\n",
    "    'BATCH_SIZE': 64,\n",
    "    'BUFFER_SIZE': 20000,\n",
    "    'NUM_LAYERS': 2,\n",
    "    'D_MODEL': 256,\n",
    "    'NUM_HEADS': 8,\n",
    "    'UNITS': 512,\n",
    "    'DROPOUT': 0.1,\n",
    "    'EPOCHS': 200\n",
    "}\n",
    "\n",
    "# 테스트할 파라미터와 값\n",
    "test_cases = {\n",
    "    'BATCH_SIZE': [32, 128],\n",
    "    'NUM_LAYERS': [3, 4],\n",
    "    'D_MODEL': [128, 512],\n",
    "    'NUM_HEADS': [4, 12]\n",
    "}\n",
    "\n",
    "# 테스트 문장\n",
    "test_sentences = [\n",
    "    \"오늘 날씨 어때?\",\n",
    "    \"나 너무 슬퍼\",\n",
    "    \"배고파\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ade7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[실험] BATCH_SIZE=32\n",
      "Epoch 1/200\n",
      "317/317 [==============================] - 13s 22ms/step - loss: 4.7688 - accuracy: 0.1374\n",
      "Epoch 2/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 3.4950 - accuracy: 0.1969\n",
      "Epoch 3/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 3.1048 - accuracy: 0.2131\n",
      "Epoch 4/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 2.8239 - accuracy: 0.2312\n",
      "Epoch 5/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 2.5301 - accuracy: 0.2578\n",
      "Epoch 6/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 2.2318 - accuracy: 0.2894\n",
      "Epoch 7/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 1.9329 - accuracy: 0.3169\n",
      "Epoch 8/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 1.6444 - accuracy: 0.3437\n",
      "Epoch 9/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 1.3766 - accuracy: 0.3727\n",
      "Epoch 10/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 1.1375 - accuracy: 0.4037\n",
      "Epoch 11/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.9230 - accuracy: 0.4363\n",
      "Epoch 12/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.7432 - accuracy: 0.4656\n",
      "Epoch 13/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.6180 - accuracy: 0.4862\n",
      "Epoch 14/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.4775 - accuracy: 0.5144\n",
      "Epoch 15/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.3638 - accuracy: 0.5379\n",
      "Epoch 16/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.2876 - accuracy: 0.5550\n",
      "Epoch 17/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.2313 - accuracy: 0.5689\n",
      "Epoch 18/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.1918 - accuracy: 0.5777\n",
      "Epoch 19/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.1643 - accuracy: 0.5853\n",
      "Epoch 20/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.1433 - accuracy: 0.5896\n",
      "Epoch 21/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.1243 - accuracy: 0.5945\n",
      "Epoch 22/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.1111 - accuracy: 0.5980\n",
      "Epoch 23/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.1049 - accuracy: 0.5994\n",
      "Epoch 24/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0910 - accuracy: 0.6032\n",
      "Epoch 25/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.0867 - accuracy: 0.6040\n",
      "Epoch 26/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.0792 - accuracy: 0.6056\n",
      "Epoch 27/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0725 - accuracy: 0.6080\n",
      "Epoch 28/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0687 - accuracy: 0.6088\n",
      "Epoch 29/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0620 - accuracy: 0.6103\n",
      "Epoch 30/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0610 - accuracy: 0.6105\n",
      "Epoch 31/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.0566 - accuracy: 0.6119\n",
      "Epoch 32/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.0548 - accuracy: 0.6119\n",
      "Epoch 33/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.0506 - accuracy: 0.6133\n",
      "Epoch 34/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.0483 - accuracy: 0.6140\n",
      "Epoch 35/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0478 - accuracy: 0.6141\n",
      "Epoch 36/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0452 - accuracy: 0.6144\n",
      "Epoch 37/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0430 - accuracy: 0.6148\n",
      "Epoch 38/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0409 - accuracy: 0.6155\n",
      "Epoch 39/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0403 - accuracy: 0.6152\n",
      "Epoch 40/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.0404 - accuracy: 0.6157\n",
      "Epoch 41/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0366 - accuracy: 0.6161\n",
      "Epoch 42/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0359 - accuracy: 0.6167\n",
      "Epoch 43/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0353 - accuracy: 0.6168\n",
      "Epoch 44/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0342 - accuracy: 0.6169\n",
      "Epoch 45/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0335 - accuracy: 0.6172\n",
      "Epoch 46/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.0320 - accuracy: 0.6174\n",
      "Epoch 47/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.0315 - accuracy: 0.6177\n",
      "Epoch 48/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0312 - accuracy: 0.6178\n",
      "Epoch 49/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0312 - accuracy: 0.6178\n",
      "Epoch 50/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0289 - accuracy: 0.6180\n",
      "Epoch 51/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.0307 - accuracy: 0.6177\n",
      "Epoch 52/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0299 - accuracy: 0.6180\n",
      "Epoch 53/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0288 - accuracy: 0.6181\n",
      "Epoch 54/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0284 - accuracy: 0.6182\n",
      "Epoch 55/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0285 - accuracy: 0.6185\n",
      "Epoch 56/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.0277 - accuracy: 0.6184\n",
      "Epoch 57/200\n",
      "317/317 [==============================] - 7s 21ms/step - loss: 0.0278 - accuracy: 0.6184\n",
      "Epoch 58/200\n",
      "317/317 [==============================] - 7s 22ms/step - loss: 0.0271 - accuracy: 0.6184\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00058: early stopping\n",
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 날씨 어플에 물어보세요 .\n",
      "입력 : 나 너무 슬퍼\n",
      "출력 : 저도 슬퍼지네요 . 기운 내길 바랄게요 .\n",
      "입력 : 배고파\n",
      "출력 : 뭐 좀 챙겨드세요 .\n",
      "\n",
      "[실험] BATCH_SIZE=128\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 8s 38ms/step - loss: 5.4740 - accuracy: 0.0460\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 4.9475 - accuracy: 0.1142\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 4.4815 - accuracy: 0.1927\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 4.0170 - accuracy: 0.1942\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 3.6344 - accuracy: 0.1948\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 3.3692 - accuracy: 0.1997\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 3s 38ms/step - loss: 3.1871 - accuracy: 0.2091\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 3.0427 - accuracy: 0.2158\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 2.9066 - accuracy: 0.2240\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 2.7733 - accuracy: 0.2347\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 2.6345 - accuracy: 0.2484\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 2.4906 - accuracy: 0.2644\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 2.3406 - accuracy: 0.2825\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 2.1878 - accuracy: 0.2988\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 2.0291 - accuracy: 0.3163\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 3s 38ms/step - loss: 1.8713 - accuracy: 0.3332\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 1.7127 - accuracy: 0.3497\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 1.5519 - accuracy: 0.3669\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 1.3929 - accuracy: 0.3856\n",
      "Epoch 20/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 3s 37ms/step - loss: 1.2369 - accuracy: 0.4070\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 1.0861 - accuracy: 0.4305\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.9433 - accuracy: 0.4530\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.8038 - accuracy: 0.4767\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.6777 - accuracy: 0.5000\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.5574 - accuracy: 0.5208\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.4531 - accuracy: 0.5413\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 3s 38ms/step - loss: 0.3596 - accuracy: 0.5605\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.2851 - accuracy: 0.5755\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.2219 - accuracy: 0.5869\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.1726 - accuracy: 0.5950\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.1388 - accuracy: 0.6000\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 3s 38ms/step - loss: 0.1145 - accuracy: 0.6031\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.0971 - accuracy: 0.6049\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 3s 38ms/step - loss: 0.0870 - accuracy: 0.6061\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 3s 38ms/step - loss: 0.0779 - accuracy: 0.6070\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.0706 - accuracy: 0.6077\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.0663 - accuracy: 0.6085\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.0615 - accuracy: 0.6092\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.0577 - accuracy: 0.6101\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.0563 - accuracy: 0.6100\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.0584 - accuracy: 0.6090\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 3s 37ms/step - loss: 0.0566 - accuracy: 0.6093\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00042: early stopping\n",
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 맛있는 거 드세요 .\n",
      "입력 : 나 너무 슬퍼\n",
      "출력 : 저도 제가 하죠 .\n",
      "입력 : 배고파\n",
      "출력 : 얼른 맛난 음식 드세요 .\n",
      "\n",
      "[실험] NUM_LAYERS=3\n",
      "Epoch 1/200\n",
      "159/159 [==============================] - 14s 36ms/step - loss: 5.1794 - accuracy: 0.0880\n",
      "Epoch 2/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 4.2801 - accuracy: 0.1926\n",
      "Epoch 3/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 3.5728 - accuracy: 0.1950\n",
      "Epoch 4/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 3.2414 - accuracy: 0.2056\n",
      "Epoch 5/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 3.0443 - accuracy: 0.2163\n",
      "Epoch 6/200\n",
      "159/159 [==============================] - 6s 36ms/step - loss: 2.8623 - accuracy: 0.2278\n",
      "Epoch 7/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 2.6742 - accuracy: 0.2431\n",
      "Epoch 8/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 2.4728 - accuracy: 0.2622\n",
      "Epoch 9/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 2.2683 - accuracy: 0.2846\n",
      "Epoch 10/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 2.0508 - accuracy: 0.3071\n",
      "Epoch 11/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 1.8325 - accuracy: 0.3297\n",
      "Epoch 12/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 1.6211 - accuracy: 0.3502\n",
      "Epoch 13/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 1.4122 - accuracy: 0.3733\n",
      "Epoch 14/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 1.2089 - accuracy: 0.3986\n",
      "Epoch 15/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 1.0170 - accuracy: 0.4276\n",
      "Epoch 16/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.8453 - accuracy: 0.4543\n",
      "Epoch 17/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.6895 - accuracy: 0.4798\n",
      "Epoch 18/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.5546 - accuracy: 0.5053\n",
      "Epoch 19/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.4510 - accuracy: 0.5245\n",
      "Epoch 20/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.3629 - accuracy: 0.5405\n",
      "Epoch 21/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.3014 - accuracy: 0.5547\n",
      "Epoch 22/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.2627 - accuracy: 0.5616\n",
      "Epoch 23/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.2320 - accuracy: 0.5674\n",
      "Epoch 24/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.2152 - accuracy: 0.5696\n",
      "Epoch 25/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.2023 - accuracy: 0.5724\n",
      "Epoch 26/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.1935 - accuracy: 0.5733\n",
      "Epoch 27/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.1629 - accuracy: 0.5812\n",
      "Epoch 28/200\n",
      "159/159 [==============================] - 5s 35ms/step - loss: 0.1460 - accuracy: 0.5855\n",
      "Epoch 29/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.1340 - accuracy: 0.5875\n",
      "Epoch 30/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.1221 - accuracy: 0.5914\n",
      "Epoch 31/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.1096 - accuracy: 0.5949\n",
      "Epoch 32/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.1014 - accuracy: 0.5964\n",
      "Epoch 33/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0927 - accuracy: 0.5990\n",
      "Epoch 34/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.0873 - accuracy: 0.6006\n",
      "Epoch 35/200\n",
      "159/159 [==============================] - 6s 36ms/step - loss: 0.0805 - accuracy: 0.6018\n",
      "Epoch 36/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0739 - accuracy: 0.6036\n",
      "Epoch 37/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0697 - accuracy: 0.6051\n",
      "Epoch 38/200\n",
      "159/159 [==============================] - 5s 35ms/step - loss: 0.0678 - accuracy: 0.6055\n",
      "Epoch 39/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0614 - accuracy: 0.6073\n",
      "Epoch 40/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0561 - accuracy: 0.6088\n",
      "Epoch 41/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0540 - accuracy: 0.6097\n",
      "Epoch 42/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0498 - accuracy: 0.6107\n",
      "Epoch 43/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0483 - accuracy: 0.6108\n",
      "Epoch 44/200\n",
      "159/159 [==============================] - 5s 35ms/step - loss: 0.0453 - accuracy: 0.6113\n",
      "Epoch 45/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0441 - accuracy: 0.6118\n",
      "Epoch 46/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0429 - accuracy: 0.6122\n",
      "Epoch 47/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0398 - accuracy: 0.6130\n",
      "Epoch 48/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0387 - accuracy: 0.6134\n",
      "Epoch 49/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0365 - accuracy: 0.6137\n",
      "Epoch 50/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0346 - accuracy: 0.6143\n",
      "Epoch 51/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0350 - accuracy: 0.6142\n",
      "Epoch 52/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.0327 - accuracy: 0.6147\n",
      "Epoch 53/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.0308 - accuracy: 0.6154\n",
      "Epoch 54/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0306 - accuracy: 0.6155\n",
      "Epoch 55/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0315 - accuracy: 0.6152\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 5s 35ms/step - loss: 0.0291 - accuracy: 0.6160\n",
      "Epoch 57/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0286 - accuracy: 0.6158\n",
      "Epoch 58/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0264 - accuracy: 0.6163\n",
      "Epoch 59/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0271 - accuracy: 0.6163\n",
      "Epoch 60/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0252 - accuracy: 0.6168\n",
      "Epoch 61/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.0260 - accuracy: 0.6166\n",
      "Epoch 62/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0230 - accuracy: 0.6172\n",
      "Epoch 63/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0240 - accuracy: 0.6172\n",
      "Epoch 64/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0217 - accuracy: 0.6178\n",
      "Epoch 65/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0233 - accuracy: 0.6169\n",
      "Epoch 66/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.0205 - accuracy: 0.6180\n",
      "Epoch 67/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.0211 - accuracy: 0.6175\n",
      "Epoch 68/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0202 - accuracy: 0.6182\n",
      "Epoch 69/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0195 - accuracy: 0.6183\n",
      "Epoch 70/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0202 - accuracy: 0.6180\n",
      "Epoch 71/200\n",
      "159/159 [==============================] - 6s 35ms/step - loss: 0.0180 - accuracy: 0.6183\n",
      "Epoch 72/200\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.0195 - accuracy: 0.6181\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00072: early stopping\n",
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 날씨 어플에 물어보세요 .\n",
      "입력 : 나 너무 슬퍼\n",
      "출력 : 하나씩 사랑이에요 .\n",
      "입력 : 배고파\n",
      "출력 : 얼른 맛난 음식 드세요 .\n",
      "\n",
      "[실험] NUM_LAYERS=4\n",
      "Epoch 1/200\n",
      "159/159 [==============================] - 17s 43ms/step - loss: 5.1791 - accuracy: 0.0798\n",
      "Epoch 2/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 4.2914 - accuracy: 0.1889\n",
      "Epoch 3/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 3.5744 - accuracy: 0.1946\n",
      "Epoch 4/200\n",
      "159/159 [==============================] - 7s 44ms/step - loss: 3.2583 - accuracy: 0.2029\n",
      "Epoch 5/200\n",
      "159/159 [==============================] - 7s 42ms/step - loss: 3.0722 - accuracy: 0.2144\n",
      "Epoch 6/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 2.9021 - accuracy: 0.2249\n",
      "Epoch 7/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 2.7242 - accuracy: 0.2375\n",
      "Epoch 8/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 2.5378 - accuracy: 0.2533\n",
      "Epoch 9/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 2.3489 - accuracy: 0.2738\n",
      "Epoch 10/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 2.1533 - accuracy: 0.2945\n",
      "Epoch 11/200\n",
      "159/159 [==============================] - 7s 42ms/step - loss: 1.9551 - accuracy: 0.3139\n",
      "Epoch 12/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 1.7622 - accuracy: 0.3322\n",
      "Epoch 13/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 1.5714 - accuracy: 0.3517\n",
      "Epoch 14/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 1.3904 - accuracy: 0.3711\n",
      "Epoch 15/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 1.2227 - accuracy: 0.3912\n",
      "Epoch 16/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 1.0640 - accuracy: 0.4128\n",
      "Epoch 17/200\n",
      "159/159 [==============================] - 7s 42ms/step - loss: 0.9135 - accuracy: 0.4356\n",
      "Epoch 18/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.7836 - accuracy: 0.4555\n",
      "Epoch 19/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.6684 - accuracy: 0.4757\n",
      "Epoch 20/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.5676 - accuracy: 0.4938\n",
      "Epoch 21/200\n",
      "159/159 [==============================] - 7s 44ms/step - loss: 0.4911 - accuracy: 0.5091\n",
      "Epoch 22/200\n",
      "159/159 [==============================] - 7s 44ms/step - loss: 0.4311 - accuracy: 0.5209\n",
      "Epoch 23/200\n",
      "159/159 [==============================] - 7s 45ms/step - loss: 0.3855 - accuracy: 0.5288\n",
      "Epoch 24/200\n",
      "159/159 [==============================] - 7s 44ms/step - loss: 0.3481 - accuracy: 0.5366\n",
      "Epoch 25/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.3218 - accuracy: 0.5424\n",
      "Epoch 26/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.2985 - accuracy: 0.5472\n",
      "Epoch 27/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.2629 - accuracy: 0.5557\n",
      "Epoch 28/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.2249 - accuracy: 0.5647\n",
      "Epoch 29/200\n",
      "159/159 [==============================] - 7s 42ms/step - loss: 0.2011 - accuracy: 0.5705\n",
      "Epoch 30/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.1792 - accuracy: 0.5762\n",
      "Epoch 31/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.1602 - accuracy: 0.5811\n",
      "Epoch 32/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.1494 - accuracy: 0.5838\n",
      "Epoch 33/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.1339 - accuracy: 0.5873\n",
      "Epoch 34/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.1247 - accuracy: 0.5908\n",
      "Epoch 35/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.1194 - accuracy: 0.5918\n",
      "Epoch 36/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.1050 - accuracy: 0.5954\n",
      "Epoch 37/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0994 - accuracy: 0.5966\n",
      "Epoch 38/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0910 - accuracy: 0.5987\n",
      "Epoch 39/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0859 - accuracy: 0.5998\n",
      "Epoch 40/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0796 - accuracy: 0.6022\n",
      "Epoch 41/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0769 - accuracy: 0.6026\n",
      "Epoch 42/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0712 - accuracy: 0.6041\n",
      "Epoch 43/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0692 - accuracy: 0.6053\n",
      "Epoch 44/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0652 - accuracy: 0.6065\n",
      "Epoch 45/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0637 - accuracy: 0.6063\n",
      "Epoch 46/200\n",
      "159/159 [==============================] - 7s 42ms/step - loss: 0.0563 - accuracy: 0.6084\n",
      "Epoch 47/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0545 - accuracy: 0.6083\n",
      "Epoch 48/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0541 - accuracy: 0.6092\n",
      "Epoch 49/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0519 - accuracy: 0.6098\n",
      "Epoch 50/200\n",
      "159/159 [==============================] - 7s 42ms/step - loss: 0.0512 - accuracy: 0.6104\n",
      "Epoch 51/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0453 - accuracy: 0.6113\n",
      "Epoch 52/200\n",
      "159/159 [==============================] - 7s 42ms/step - loss: 0.0458 - accuracy: 0.6112\n",
      "Epoch 53/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0431 - accuracy: 0.6123\n",
      "Epoch 54/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0439 - accuracy: 0.6120\n",
      "Epoch 55/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0397 - accuracy: 0.6129\n",
      "Epoch 56/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0408 - accuracy: 0.6126\n",
      "Epoch 57/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0376 - accuracy: 0.6130\n",
      "Epoch 58/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0376 - accuracy: 0.6138\n",
      "Epoch 59/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0356 - accuracy: 0.6138\n",
      "Epoch 60/200\n",
      "159/159 [==============================] - 7s 44ms/step - loss: 0.0364 - accuracy: 0.6139\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 7s 44ms/step - loss: 0.0324 - accuracy: 0.6147\n",
      "Epoch 62/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0309 - accuracy: 0.6150\n",
      "Epoch 63/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0315 - accuracy: 0.6147\n",
      "Epoch 64/200\n",
      "159/159 [==============================] - 7s 42ms/step - loss: 0.0298 - accuracy: 0.6151\n",
      "Epoch 65/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0310 - accuracy: 0.6151\n",
      "Epoch 66/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0299 - accuracy: 0.6153\n",
      "Epoch 67/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0276 - accuracy: 0.6160\n",
      "Epoch 68/200\n",
      "159/159 [==============================] - 7s 44ms/step - loss: 0.0267 - accuracy: 0.6161\n",
      "Epoch 69/200\n",
      "159/159 [==============================] - 7s 44ms/step - loss: 0.0270 - accuracy: 0.6163\n",
      "Epoch 70/200\n",
      "159/159 [==============================] - 7s 44ms/step - loss: 0.0256 - accuracy: 0.6166\n",
      "Epoch 71/200\n",
      "159/159 [==============================] - 7s 44ms/step - loss: 0.0257 - accuracy: 0.6165\n",
      "Epoch 72/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0266 - accuracy: 0.6167\n",
      "Epoch 73/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0255 - accuracy: 0.6165\n",
      "Epoch 74/200\n",
      "159/159 [==============================] - 7s 44ms/step - loss: 0.0240 - accuracy: 0.6170\n",
      "Epoch 75/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0240 - accuracy: 0.6166\n",
      "Epoch 76/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0220 - accuracy: 0.6177\n",
      "Epoch 77/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0230 - accuracy: 0.6174\n",
      "Epoch 78/200\n",
      "159/159 [==============================] - 7s 43ms/step - loss: 0.0220 - accuracy: 0.6174\n",
      "Epoch 79/200\n",
      "159/159 [==============================] - 7s 44ms/step - loss: 0.0214 - accuracy: 0.6176\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00079: early stopping\n",
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 친구들과 같이 놀러가세요 .\n",
      "입력 : 나 너무 슬퍼\n",
      "출력 : 저도 행동 싶네요 .\n",
      "입력 : 배고파\n",
      "출력 : 뭐 좀 챙겨드세요 .\n",
      "\n",
      "[실험] D_MODEL=128\n",
      "Epoch 1/200\n",
      "159/159 [==============================] - 10s 25ms/step - loss: 5.3766 - accuracy: 0.0818\n",
      "Epoch 2/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 4.7541 - accuracy: 0.1792\n",
      "Epoch 3/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 3.9938 - accuracy: 0.1941\n",
      "Epoch 4/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 3.4532 - accuracy: 0.1951\n",
      "Epoch 5/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 3.1698 - accuracy: 0.2061\n",
      "Epoch 6/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 2.9896 - accuracy: 0.2175\n",
      "Epoch 7/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 2.8232 - accuracy: 0.2295\n",
      "Epoch 8/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 2.6442 - accuracy: 0.2446\n",
      "Epoch 9/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 2.4532 - accuracy: 0.2640\n",
      "Epoch 10/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 2.2518 - accuracy: 0.2862\n",
      "Epoch 11/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 2.0462 - accuracy: 0.3074\n",
      "Epoch 12/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 1.8294 - accuracy: 0.3302\n",
      "Epoch 13/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 1.6120 - accuracy: 0.3541\n",
      "Epoch 14/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 1.4059 - accuracy: 0.3765\n",
      "Epoch 15/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 1.2053 - accuracy: 0.4011\n",
      "Epoch 16/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 1.0210 - accuracy: 0.4273\n",
      "Epoch 17/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.8564 - accuracy: 0.4518\n",
      "Epoch 18/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.7033 - accuracy: 0.4786\n",
      "Epoch 19/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.5772 - accuracy: 0.4994\n",
      "Epoch 20/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.4745 - accuracy: 0.5178\n",
      "Epoch 21/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.3829 - accuracy: 0.5367\n",
      "Epoch 22/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.3223 - accuracy: 0.5476\n",
      "Epoch 23/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.2785 - accuracy: 0.5559\n",
      "Epoch 24/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.2411 - accuracy: 0.5633\n",
      "Epoch 25/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.2201 - accuracy: 0.5675\n",
      "Epoch 26/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.1995 - accuracy: 0.5720\n",
      "Epoch 27/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.1748 - accuracy: 0.5777\n",
      "Epoch 28/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.1515 - accuracy: 0.5835\n",
      "Epoch 29/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.1347 - accuracy: 0.5884\n",
      "Epoch 30/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.1223 - accuracy: 0.5913\n",
      "Epoch 31/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.1124 - accuracy: 0.5928\n",
      "Epoch 32/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0977 - accuracy: 0.5976\n",
      "Epoch 33/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0927 - accuracy: 0.5989\n",
      "Epoch 34/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0852 - accuracy: 0.6009\n",
      "Epoch 35/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0772 - accuracy: 0.6029\n",
      "Epoch 36/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0714 - accuracy: 0.6047\n",
      "Epoch 37/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0684 - accuracy: 0.6049\n",
      "Epoch 38/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0624 - accuracy: 0.6069\n",
      "Epoch 39/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0592 - accuracy: 0.6078\n",
      "Epoch 40/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0535 - accuracy: 0.6093\n",
      "Epoch 41/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0520 - accuracy: 0.6095\n",
      "Epoch 42/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0470 - accuracy: 0.6110\n",
      "Epoch 43/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.0470 - accuracy: 0.6108\n",
      "Epoch 44/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0466 - accuracy: 0.6106\n",
      "Epoch 45/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0396 - accuracy: 0.6130\n",
      "Epoch 46/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0428 - accuracy: 0.6120\n",
      "Epoch 47/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0391 - accuracy: 0.6133\n",
      "Epoch 48/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.0358 - accuracy: 0.6138\n",
      "Epoch 49/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0341 - accuracy: 0.6144\n",
      "Epoch 50/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.0338 - accuracy: 0.6144\n",
      "Epoch 51/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0338 - accuracy: 0.6143\n",
      "Epoch 52/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0322 - accuracy: 0.6147\n",
      "Epoch 53/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0303 - accuracy: 0.6158\n",
      "Epoch 54/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0289 - accuracy: 0.6157\n",
      "Epoch 55/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0287 - accuracy: 0.6156\n",
      "Epoch 56/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.0257 - accuracy: 0.6162\n",
      "Epoch 57/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.0271 - accuracy: 0.6163\n",
      "Epoch 58/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.0252 - accuracy: 0.6167\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 4s 25ms/step - loss: 0.0251 - accuracy: 0.6165\n",
      "Epoch 60/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.0236 - accuracy: 0.6169\n",
      "Epoch 61/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.0233 - accuracy: 0.6172\n",
      "Epoch 62/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0233 - accuracy: 0.6170\n",
      "Epoch 63/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0219 - accuracy: 0.6174\n",
      "Epoch 64/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0225 - accuracy: 0.6176\n",
      "Epoch 65/200\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.0203 - accuracy: 0.6181\n",
      "Epoch 66/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0196 - accuracy: 0.6181\n",
      "Epoch 67/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0209 - accuracy: 0.6173\n",
      "Epoch 68/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0188 - accuracy: 0.6184\n",
      "Epoch 69/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0199 - accuracy: 0.6178\n",
      "Epoch 70/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0181 - accuracy: 0.6183\n",
      "Epoch 71/200\n",
      "159/159 [==============================] - 4s 24ms/step - loss: 0.0176 - accuracy: 0.6182\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00071: early stopping\n",
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 날씨 어플에 물어보세요 .\n",
      "입력 : 나 너무 슬퍼\n",
      "출력 : 저도 간절히 기도 할게요 .\n",
      "입력 : 배고파\n",
      "출력 : 얼른 맛난 음식 드세요 .\n",
      "\n",
      "[실험] D_MODEL=512\n",
      "Epoch 1/200\n",
      "159/159 [==============================] - 11s 38ms/step - loss: 4.8778 - accuracy: 0.1315\n",
      "Epoch 2/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 3.7368 - accuracy: 0.1937\n",
      "Epoch 3/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 3.3215 - accuracy: 0.2025\n",
      "Epoch 4/200\n",
      "159/159 [==============================] - 6s 38ms/step - loss: 3.0820 - accuracy: 0.2144\n",
      "Epoch 5/200\n",
      "159/159 [==============================] - 6s 38ms/step - loss: 2.8554 - accuracy: 0.2292\n",
      "Epoch 6/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 2.6168 - accuracy: 0.2500\n",
      "Epoch 7/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 2.3654 - accuracy: 0.2766\n",
      "Epoch 8/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 2.1025 - accuracy: 0.3036\n",
      "Epoch 9/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 1.8320 - accuracy: 0.3301\n",
      "Epoch 10/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 1.5691 - accuracy: 0.3572\n",
      "Epoch 11/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 1.3135 - accuracy: 0.3893\n",
      "Epoch 12/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 1.0738 - accuracy: 0.4242\n",
      "Epoch 13/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 0.8529 - accuracy: 0.4606\n",
      "Epoch 14/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 0.6542 - accuracy: 0.4959\n",
      "Epoch 15/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 0.4837 - accuracy: 0.5295\n",
      "Epoch 16/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 0.3516 - accuracy: 0.5566\n",
      "Epoch 17/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 0.2508 - accuracy: 0.5766\n",
      "Epoch 18/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 0.1897 - accuracy: 0.5876\n",
      "Epoch 19/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 0.1528 - accuracy: 0.5922\n",
      "Epoch 20/200\n",
      "159/159 [==============================] - 6s 38ms/step - loss: 0.1363 - accuracy: 0.5941\n",
      "Epoch 21/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 0.1262 - accuracy: 0.5955\n",
      "Epoch 22/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 0.1178 - accuracy: 0.5961\n",
      "Epoch 23/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 0.1185 - accuracy: 0.5954\n",
      "Epoch 24/200\n",
      "159/159 [==============================] - 6s 38ms/step - loss: 0.1183 - accuracy: 0.5946\n",
      "Epoch 25/200\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 0.1151 - accuracy: 0.5960\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00025: early stopping\n",
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 사랑에 자격이 있어요 .\n",
      "입력 : 나 너무 슬퍼\n",
      "출력 : 잘하고 있어요 .\n",
      "입력 : 배고파\n",
      "출력 : 맛난 음식 드세요 .\n",
      "\n",
      "[실험] NUM_HEADS=4\n",
      "Epoch 1/200\n",
      "159/159 [==============================] - 10s 27ms/step - loss: 5.1678 - accuracy: 0.1025\n",
      "Epoch 2/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 4.2428 - accuracy: 0.1923\n",
      "Epoch 3/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 3.5494 - accuracy: 0.1954\n",
      "Epoch 4/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 3.2238 - accuracy: 0.2068\n",
      "Epoch 5/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 3.0146 - accuracy: 0.2167\n",
      "Epoch 6/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 2.8183 - accuracy: 0.2313\n",
      "Epoch 7/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 2.6121 - accuracy: 0.2505\n",
      "Epoch 8/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 2.3943 - accuracy: 0.2736\n",
      "Epoch 9/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 2.1685 - accuracy: 0.2976\n",
      "Epoch 10/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 1.9372 - accuracy: 0.3198\n",
      "Epoch 11/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 1.7020 - accuracy: 0.3443\n",
      "Epoch 12/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 1.4703 - accuracy: 0.3680\n",
      "Epoch 13/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 1.2521 - accuracy: 0.3951\n",
      "Epoch 14/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 1.0436 - accuracy: 0.4245\n",
      "Epoch 15/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.8502 - accuracy: 0.4563\n",
      "Epoch 16/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.6782 - accuracy: 0.4855\n",
      "Epoch 17/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.5299 - accuracy: 0.5135\n",
      "Epoch 18/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.4062 - accuracy: 0.5366\n",
      "Epoch 19/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.3123 - accuracy: 0.5561\n",
      "Epoch 20/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.2479 - accuracy: 0.5691\n",
      "Epoch 21/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.2017 - accuracy: 0.5776\n",
      "Epoch 22/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1781 - accuracy: 0.5821\n",
      "Epoch 23/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1623 - accuracy: 0.5840\n",
      "Epoch 24/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1507 - accuracy: 0.5864\n",
      "Epoch 25/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1466 - accuracy: 0.5867\n",
      "Epoch 26/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.1382 - accuracy: 0.5878\n",
      "Epoch 27/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1180 - accuracy: 0.5927\n",
      "Epoch 28/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.1094 - accuracy: 0.5950\n",
      "Epoch 29/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0972 - accuracy: 0.5987\n",
      "Epoch 30/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0870 - accuracy: 0.6017\n",
      "Epoch 31/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0785 - accuracy: 0.6033\n",
      "Epoch 32/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0744 - accuracy: 0.6046\n",
      "Epoch 33/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0688 - accuracy: 0.6058\n",
      "Epoch 34/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0624 - accuracy: 0.6073\n",
      "Epoch 35/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0568 - accuracy: 0.6089\n",
      "Epoch 36/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0545 - accuracy: 0.6096\n",
      "Epoch 37/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0505 - accuracy: 0.6104\n",
      "Epoch 38/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0467 - accuracy: 0.6114\n",
      "Epoch 39/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0440 - accuracy: 0.6122\n",
      "Epoch 40/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0450 - accuracy: 0.6122\n",
      "Epoch 41/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0406 - accuracy: 0.6129\n",
      "Epoch 42/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0386 - accuracy: 0.6134\n",
      "Epoch 43/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0369 - accuracy: 0.6140\n",
      "Epoch 44/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0340 - accuracy: 0.6147\n",
      "Epoch 45/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0335 - accuracy: 0.6148\n",
      "Epoch 46/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0329 - accuracy: 0.6151\n",
      "Epoch 47/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0299 - accuracy: 0.6154\n",
      "Epoch 48/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0294 - accuracy: 0.6156\n",
      "Epoch 49/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0294 - accuracy: 0.6157\n",
      "Epoch 50/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0265 - accuracy: 0.6167\n",
      "Epoch 51/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0256 - accuracy: 0.6165\n",
      "Epoch 52/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0264 - accuracy: 0.6165\n",
      "Epoch 53/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0242 - accuracy: 0.6169\n",
      "Epoch 54/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0228 - accuracy: 0.6175\n",
      "Epoch 55/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0226 - accuracy: 0.6176\n",
      "Epoch 56/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0204 - accuracy: 0.6179\n",
      "Epoch 57/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0221 - accuracy: 0.6177\n",
      "Epoch 58/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0205 - accuracy: 0.6179\n",
      "Epoch 59/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0204 - accuracy: 0.6178\n",
      "Epoch 60/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0189 - accuracy: 0.6184\n",
      "Epoch 61/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0196 - accuracy: 0.6181\n",
      "Epoch 62/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0182 - accuracy: 0.6184\n",
      "Epoch 63/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0170 - accuracy: 0.6187\n",
      "Epoch 64/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0169 - accuracy: 0.6190\n",
      "Epoch 65/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0173 - accuracy: 0.6185\n",
      "Epoch 66/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0149 - accuracy: 0.6192\n",
      "Epoch 67/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0166 - accuracy: 0.6186\n",
      "Epoch 68/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0166 - accuracy: 0.6186\n",
      "Epoch 69/200\n",
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0153 - accuracy: 0.6191\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00069: early stopping\n",
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 날씨 어플에 물어보세요 .\n",
      "입력 : 나 너무 슬퍼\n",
      "출력 : 당신만이 정답을 맞을 주세요 .\n",
      "입력 : 배고파\n",
      "출력 : 얼른 맛난 음식 드세요 .\n",
      "\n",
      "[실험] NUM_HEADS=12\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/4261045313.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[실험] {param}={value}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/4219450454.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(parameter_dic)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mDROPOUT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DROPOUT'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 드롭아웃의 비율\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     model = transformer(\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/1349913387.py\u001b[0m in \u001b[0;36mtransformer\u001b[0;34m(vocab_size, num_layers, units, d_model, num_heads, dropout, name)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# 인코더\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     enc_outputs = encoder(\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/375079122.py\u001b[0m in \u001b[0;36mencoder\u001b[0;34m(vocab_size, num_layers, units, d_model, num_heads, dropout, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# num_layers만큼 쌓아올린 인코더의 층.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         outputs = encoder_layer(\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/268519183.py\u001b[0m in \u001b[0;36mencoder_layer\u001b[0;34m(units, d_model, num_heads, dropout, name)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     attention = MultiHeadAttention(\n\u001b[0m\u001b[1;32m      9\u001b[0m       d_model, num_heads, name=\"attention\")({\n\u001b[1;32m     10\u001b[0m           \u001b[0;34m'query'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/1099654616.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d_model, num_heads, name)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0md_model\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment_results = []\n",
    "\n",
    "for param, values in test_cases.items():\n",
    "    for value in values:\n",
    "        params = base_params.copy()\n",
    "        params[param] = value\n",
    "        \n",
    "        print(f\"\\n[실험] {param}={value}\")\n",
    "        model = train_model(params)\n",
    "        \n",
    "        for sentence in test_sentences:\n",
    "            output = sentence_generation(sentence, model)\n",
    "            experiment_results.append({\n",
    "                'Tested_Parameter': param,\n",
    "                'Value': value,\n",
    "                'Input_Sentence': sentence,\n",
    "                'Output_Response': output\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list_1 = experiment_results.copy() # 결과값 백업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드에러 2\n",
    "'''\n",
    " assert d_model % self.num_heads == 0\n",
    "'D_MODEL' : 256, # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "256/12 = 0 되지않아서 오류 발생\n",
    "이후 파라미터 수정\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05229b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = {\n",
    "    'BATCH_SIZE': 64,\n",
    "    'BUFFER_SIZE': 20000,\n",
    "    'NUM_LAYERS': 2,\n",
    "    'D_MODEL': 256,\n",
    "    'NUM_HEADS': 16, # 16으로 변경 후 진행행\n",
    "    'UNITS': 512,\n",
    "    'DROPOUT': 0.1,\n",
    "    'EPOCHS': 200\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "400ba31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "159/159 [==============================] - 9s 28ms/step - loss: 5.1997 - accuracy: 0.1106\n",
      "Epoch 2/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 4.3015 - accuracy: 0.1923\n",
      "Epoch 3/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 3.5853 - accuracy: 0.1950\n",
      "Epoch 4/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 3.2398 - accuracy: 0.2040\n",
      "Epoch 5/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 3.0319 - accuracy: 0.2162\n",
      "Epoch 6/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 2.8427 - accuracy: 0.2287\n",
      "Epoch 7/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 2.6398 - accuracy: 0.2471\n",
      "Epoch 8/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 2.4197 - accuracy: 0.2716\n",
      "Epoch 9/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 2.1871 - accuracy: 0.2963\n",
      "Epoch 10/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 1.9450 - accuracy: 0.3214\n",
      "Epoch 11/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 1.6945 - accuracy: 0.3477\n",
      "Epoch 12/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 1.4527 - accuracy: 0.3751\n",
      "Epoch 13/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 1.2196 - accuracy: 0.4039\n",
      "Epoch 14/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.9995 - accuracy: 0.4371\n",
      "Epoch 15/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.7969 - accuracy: 0.4697\n",
      "Epoch 16/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.6223 - accuracy: 0.4999\n",
      "Epoch 17/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.4715 - accuracy: 0.5286\n",
      "Epoch 18/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.3576 - accuracy: 0.5507\n",
      "Epoch 19/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.2676 - accuracy: 0.5679\n",
      "Epoch 20/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.2111 - accuracy: 0.5786\n",
      "Epoch 21/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1781 - accuracy: 0.5832\n",
      "Epoch 22/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1560 - accuracy: 0.5871\n",
      "Epoch 23/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1414 - accuracy: 0.5891\n",
      "Epoch 24/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1388 - accuracy: 0.5896\n",
      "Epoch 25/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1315 - accuracy: 0.5904\n",
      "Epoch 26/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1241 - accuracy: 0.5917\n",
      "Epoch 27/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.1119 - accuracy: 0.5946\n",
      "Epoch 28/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0995 - accuracy: 0.5983\n",
      "Epoch 29/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0867 - accuracy: 0.6014\n",
      "Epoch 30/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0787 - accuracy: 0.6036\n",
      "Epoch 31/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0743 - accuracy: 0.6043\n",
      "Epoch 32/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0660 - accuracy: 0.6064\n",
      "Epoch 33/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0596 - accuracy: 0.6082\n",
      "Epoch 34/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0588 - accuracy: 0.6086\n",
      "Epoch 35/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0513 - accuracy: 0.6099\n",
      "Epoch 36/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0482 - accuracy: 0.6110\n",
      "Epoch 37/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0445 - accuracy: 0.6121\n",
      "Epoch 38/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0432 - accuracy: 0.6126\n",
      "Epoch 39/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0417 - accuracy: 0.6128\n",
      "Epoch 40/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0371 - accuracy: 0.6138\n",
      "Epoch 41/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0361 - accuracy: 0.6141\n",
      "Epoch 42/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0339 - accuracy: 0.6148\n",
      "Epoch 43/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0319 - accuracy: 0.6152\n",
      "Epoch 44/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0302 - accuracy: 0.6156\n",
      "Epoch 45/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0307 - accuracy: 0.6156\n",
      "Epoch 46/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0276 - accuracy: 0.6164\n",
      "Epoch 47/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0256 - accuracy: 0.6167\n",
      "Epoch 48/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0249 - accuracy: 0.6171\n",
      "Epoch 49/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0263 - accuracy: 0.6166\n",
      "Epoch 50/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0234 - accuracy: 0.6174\n",
      "Epoch 51/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0231 - accuracy: 0.6174\n",
      "Epoch 52/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0216 - accuracy: 0.6178\n",
      "Epoch 53/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0211 - accuracy: 0.6179\n",
      "Epoch 54/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0204 - accuracy: 0.6181\n",
      "Epoch 55/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0201 - accuracy: 0.6183\n",
      "Epoch 56/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0195 - accuracy: 0.6185\n",
      "Epoch 57/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0172 - accuracy: 0.6188\n",
      "Epoch 58/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0181 - accuracy: 0.6187\n",
      "Epoch 59/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0167 - accuracy: 0.6189\n",
      "Epoch 60/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0174 - accuracy: 0.6188\n",
      "Epoch 61/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0154 - accuracy: 0.6191\n",
      "Epoch 62/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0152 - accuracy: 0.6192\n",
      "Epoch 63/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0153 - accuracy: 0.6193\n",
      "Epoch 64/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0156 - accuracy: 0.6190\n",
      "Epoch 65/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0143 - accuracy: 0.6193\n",
      "Epoch 66/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0142 - accuracy: 0.6195\n",
      "Epoch 67/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0129 - accuracy: 0.6194\n",
      "Epoch 68/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0138 - accuracy: 0.6195\n",
      "Epoch 69/200\n",
      "159/159 [==============================] - 4s 27ms/step - loss: 0.0136 - accuracy: 0.6195\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00069: early stopping\n",
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 날씨 어플에 물어보세요 .\n",
      "입력 : 나 너무 슬퍼\n",
      "출력 : 당신만이 정답을 맞을 주세요 .\n",
      "입력 : 배고파\n",
      "출력 : 얼른 맛난 음식 드세요 .\n"
     ]
    }
   ],
   "source": [
    "test_model = train_model(test_params)\n",
    "for sentence in test_sentences:\n",
    "    output = sentence_generation(sentence, model)\n",
    "    experiment_results.append({\n",
    "        'Tested_Parameter': param,\n",
    "        'Value': value,\n",
    "        'Input_Sentence': sentence,\n",
    "        'Output_Response': output\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "312c816d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Tested_Parameter': 'BATCH_SIZE',\n",
       "  'Value': 32,\n",
       "  'Input_Sentence': '오늘 날씨 어때?',\n",
       "  'Output_Response': '날씨 어플에 물어보세요 .'},\n",
       " {'Tested_Parameter': 'BATCH_SIZE',\n",
       "  'Value': 32,\n",
       "  'Input_Sentence': '나 너무 슬퍼',\n",
       "  'Output_Response': '저도 슬퍼지네요 . 기운 내길 바랄게요 .'},\n",
       " {'Tested_Parameter': 'BATCH_SIZE',\n",
       "  'Value': 32,\n",
       "  'Input_Sentence': '배고파',\n",
       "  'Output_Response': '뭐 좀 챙겨드세요 .'},\n",
       " {'Tested_Parameter': 'BATCH_SIZE',\n",
       "  'Value': 128,\n",
       "  'Input_Sentence': '오늘 날씨 어때?',\n",
       "  'Output_Response': '맛있는 거 드세요 .'},\n",
       " {'Tested_Parameter': 'BATCH_SIZE',\n",
       "  'Value': 128,\n",
       "  'Input_Sentence': '나 너무 슬퍼',\n",
       "  'Output_Response': '저도 제가 하죠 .'},\n",
       " {'Tested_Parameter': 'BATCH_SIZE',\n",
       "  'Value': 128,\n",
       "  'Input_Sentence': '배고파',\n",
       "  'Output_Response': '얼른 맛난 음식 드세요 .'},\n",
       " {'Tested_Parameter': 'NUM_LAYERS',\n",
       "  'Value': 3,\n",
       "  'Input_Sentence': '오늘 날씨 어때?',\n",
       "  'Output_Response': '날씨 어플에 물어보세요 .'},\n",
       " {'Tested_Parameter': 'NUM_LAYERS',\n",
       "  'Value': 3,\n",
       "  'Input_Sentence': '나 너무 슬퍼',\n",
       "  'Output_Response': '하나씩 사랑이에요 .'},\n",
       " {'Tested_Parameter': 'NUM_LAYERS',\n",
       "  'Value': 3,\n",
       "  'Input_Sentence': '배고파',\n",
       "  'Output_Response': '얼른 맛난 음식 드세요 .'},\n",
       " {'Tested_Parameter': 'NUM_LAYERS',\n",
       "  'Value': 4,\n",
       "  'Input_Sentence': '오늘 날씨 어때?',\n",
       "  'Output_Response': '친구들과 같이 놀러가세요 .'},\n",
       " {'Tested_Parameter': 'NUM_LAYERS',\n",
       "  'Value': 4,\n",
       "  'Input_Sentence': '나 너무 슬퍼',\n",
       "  'Output_Response': '저도 행동 싶네요 .'},\n",
       " {'Tested_Parameter': 'NUM_LAYERS',\n",
       "  'Value': 4,\n",
       "  'Input_Sentence': '배고파',\n",
       "  'Output_Response': '뭐 좀 챙겨드세요 .'},\n",
       " {'Tested_Parameter': 'D_MODEL',\n",
       "  'Value': 128,\n",
       "  'Input_Sentence': '오늘 날씨 어때?',\n",
       "  'Output_Response': '날씨 어플에 물어보세요 .'},\n",
       " {'Tested_Parameter': 'D_MODEL',\n",
       "  'Value': 128,\n",
       "  'Input_Sentence': '나 너무 슬퍼',\n",
       "  'Output_Response': '저도 간절히 기도 할게요 .'},\n",
       " {'Tested_Parameter': 'D_MODEL',\n",
       "  'Value': 128,\n",
       "  'Input_Sentence': '배고파',\n",
       "  'Output_Response': '얼른 맛난 음식 드세요 .'},\n",
       " {'Tested_Parameter': 'D_MODEL',\n",
       "  'Value': 512,\n",
       "  'Input_Sentence': '오늘 날씨 어때?',\n",
       "  'Output_Response': '사랑에 자격이 있어요 .'},\n",
       " {'Tested_Parameter': 'D_MODEL',\n",
       "  'Value': 512,\n",
       "  'Input_Sentence': '나 너무 슬퍼',\n",
       "  'Output_Response': '잘하고 있어요 .'},\n",
       " {'Tested_Parameter': 'D_MODEL',\n",
       "  'Value': 512,\n",
       "  'Input_Sentence': '배고파',\n",
       "  'Output_Response': '맛난 음식 드세요 .'},\n",
       " {'Tested_Parameter': 'NUM_HEADS',\n",
       "  'Value': 4,\n",
       "  'Input_Sentence': '오늘 날씨 어때?',\n",
       "  'Output_Response': '날씨 어플에 물어보세요 .'},\n",
       " {'Tested_Parameter': 'NUM_HEADS',\n",
       "  'Value': 4,\n",
       "  'Input_Sentence': '나 너무 슬퍼',\n",
       "  'Output_Response': '당신만이 정답을 맞을 주세요 .'},\n",
       " {'Tested_Parameter': 'NUM_HEADS',\n",
       "  'Value': 4,\n",
       "  'Input_Sentence': '배고파',\n",
       "  'Output_Response': '얼른 맛난 음식 드세요 .'},\n",
       " {'Tested_Parameter': 'NUM_HEADS',\n",
       "  'Value': 12,\n",
       "  'Input_Sentence': '오늘 날씨 어때?',\n",
       "  'Output_Response': '날씨 어플에 물어보세요 .'},\n",
       " {'Tested_Parameter': 'NUM_HEADS',\n",
       "  'Value': 12,\n",
       "  'Input_Sentence': '나 너무 슬퍼',\n",
       "  'Output_Response': '당신만이 정답을 맞을 주세요 .'},\n",
       " {'Tested_Parameter': 'NUM_HEADS',\n",
       "  'Value': 12,\n",
       "  'Input_Sentence': '배고파',\n",
       "  'Output_Response': '얼른 맛난 음식 드세요 .'}]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b42c7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(experiment_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7145f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tested_Parameter</th>\n",
       "      <th>Value</th>\n",
       "      <th>Input_Sentence</th>\n",
       "      <th>Output_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BATCH_SIZE</td>\n",
       "      <td>32</td>\n",
       "      <td>오늘 날씨 어때?</td>\n",
       "      <td>날씨 어플에 물어보세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BATCH_SIZE</td>\n",
       "      <td>32</td>\n",
       "      <td>나 너무 슬퍼</td>\n",
       "      <td>저도 슬퍼지네요 . 기운 내길 바랄게요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BATCH_SIZE</td>\n",
       "      <td>32</td>\n",
       "      <td>배고파</td>\n",
       "      <td>뭐 좀 챙겨드세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BATCH_SIZE</td>\n",
       "      <td>128</td>\n",
       "      <td>오늘 날씨 어때?</td>\n",
       "      <td>맛있는 거 드세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BATCH_SIZE</td>\n",
       "      <td>128</td>\n",
       "      <td>나 너무 슬퍼</td>\n",
       "      <td>저도 제가 하죠 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BATCH_SIZE</td>\n",
       "      <td>128</td>\n",
       "      <td>배고파</td>\n",
       "      <td>얼른 맛난 음식 드세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NUM_LAYERS</td>\n",
       "      <td>3</td>\n",
       "      <td>오늘 날씨 어때?</td>\n",
       "      <td>날씨 어플에 물어보세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NUM_LAYERS</td>\n",
       "      <td>3</td>\n",
       "      <td>나 너무 슬퍼</td>\n",
       "      <td>하나씩 사랑이에요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NUM_LAYERS</td>\n",
       "      <td>3</td>\n",
       "      <td>배고파</td>\n",
       "      <td>얼른 맛난 음식 드세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NUM_LAYERS</td>\n",
       "      <td>4</td>\n",
       "      <td>오늘 날씨 어때?</td>\n",
       "      <td>친구들과 같이 놀러가세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NUM_LAYERS</td>\n",
       "      <td>4</td>\n",
       "      <td>나 너무 슬퍼</td>\n",
       "      <td>저도 행동 싶네요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NUM_LAYERS</td>\n",
       "      <td>4</td>\n",
       "      <td>배고파</td>\n",
       "      <td>뭐 좀 챙겨드세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>D_MODEL</td>\n",
       "      <td>128</td>\n",
       "      <td>오늘 날씨 어때?</td>\n",
       "      <td>날씨 어플에 물어보세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>D_MODEL</td>\n",
       "      <td>128</td>\n",
       "      <td>나 너무 슬퍼</td>\n",
       "      <td>저도 간절히 기도 할게요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>D_MODEL</td>\n",
       "      <td>128</td>\n",
       "      <td>배고파</td>\n",
       "      <td>얼른 맛난 음식 드세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D_MODEL</td>\n",
       "      <td>512</td>\n",
       "      <td>오늘 날씨 어때?</td>\n",
       "      <td>사랑에 자격이 있어요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D_MODEL</td>\n",
       "      <td>512</td>\n",
       "      <td>나 너무 슬퍼</td>\n",
       "      <td>잘하고 있어요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>D_MODEL</td>\n",
       "      <td>512</td>\n",
       "      <td>배고파</td>\n",
       "      <td>맛난 음식 드세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NUM_HEADS</td>\n",
       "      <td>4</td>\n",
       "      <td>오늘 날씨 어때?</td>\n",
       "      <td>날씨 어플에 물어보세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NUM_HEADS</td>\n",
       "      <td>4</td>\n",
       "      <td>나 너무 슬퍼</td>\n",
       "      <td>당신만이 정답을 맞을 주세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NUM_HEADS</td>\n",
       "      <td>4</td>\n",
       "      <td>배고파</td>\n",
       "      <td>얼른 맛난 음식 드세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NUM_HEADS</td>\n",
       "      <td>12</td>\n",
       "      <td>오늘 날씨 어때?</td>\n",
       "      <td>날씨 어플에 물어보세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NUM_HEADS</td>\n",
       "      <td>12</td>\n",
       "      <td>나 너무 슬퍼</td>\n",
       "      <td>당신만이 정답을 맞을 주세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NUM_HEADS</td>\n",
       "      <td>12</td>\n",
       "      <td>배고파</td>\n",
       "      <td>얼른 맛난 음식 드세요 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tested_Parameter  Value Input_Sentence          Output_Response\n",
       "0        BATCH_SIZE     32      오늘 날씨 어때?           날씨 어플에 물어보세요 .\n",
       "1        BATCH_SIZE     32        나 너무 슬퍼  저도 슬퍼지네요 . 기운 내길 바랄게요 .\n",
       "2        BATCH_SIZE     32            배고파              뭐 좀 챙겨드세요 .\n",
       "3        BATCH_SIZE    128      오늘 날씨 어때?              맛있는 거 드세요 .\n",
       "4        BATCH_SIZE    128        나 너무 슬퍼               저도 제가 하죠 .\n",
       "5        BATCH_SIZE    128            배고파           얼른 맛난 음식 드세요 .\n",
       "6        NUM_LAYERS      3      오늘 날씨 어때?           날씨 어플에 물어보세요 .\n",
       "7        NUM_LAYERS      3        나 너무 슬퍼              하나씩 사랑이에요 .\n",
       "8        NUM_LAYERS      3            배고파           얼른 맛난 음식 드세요 .\n",
       "9        NUM_LAYERS      4      오늘 날씨 어때?          친구들과 같이 놀러가세요 .\n",
       "10       NUM_LAYERS      4        나 너무 슬퍼              저도 행동 싶네요 .\n",
       "11       NUM_LAYERS      4            배고파              뭐 좀 챙겨드세요 .\n",
       "12          D_MODEL    128      오늘 날씨 어때?           날씨 어플에 물어보세요 .\n",
       "13          D_MODEL    128        나 너무 슬퍼          저도 간절히 기도 할게요 .\n",
       "14          D_MODEL    128            배고파           얼른 맛난 음식 드세요 .\n",
       "15          D_MODEL    512      오늘 날씨 어때?            사랑에 자격이 있어요 .\n",
       "16          D_MODEL    512        나 너무 슬퍼                잘하고 있어요 .\n",
       "17          D_MODEL    512            배고파              맛난 음식 드세요 .\n",
       "18        NUM_HEADS      4      오늘 날씨 어때?           날씨 어플에 물어보세요 .\n",
       "19        NUM_HEADS      4        나 너무 슬퍼        당신만이 정답을 맞을 주세요 .\n",
       "20        NUM_HEADS      4            배고파           얼른 맛난 음식 드세요 .\n",
       "21        NUM_HEADS     12      오늘 날씨 어때?           날씨 어플에 물어보세요 .\n",
       "22        NUM_HEADS     12        나 너무 슬퍼        당신만이 정답을 맞을 주세요 .\n",
       "23        NUM_HEADS     12            배고파           얼른 맛난 음식 드세요 ."
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bdcbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 32 성능 준수\n",
    "# 배치 128 성능 별로\n",
    "# 배치가 작을수록 성능 우수\n",
    "\n",
    "# 레이어갯수(인코더, 디코더의 레이어 수) 3 성능 어색\n",
    "# 레이어갯수(인코더, 디코더의 레이어 수) 4 성능 이상\n",
    "# 레이어 깊어질수록 다양한 단어?는 사용하는거 같은데 질문에 맞는 답변은 아님\n",
    "\n",
    "# D_model(차원 수) 128 낫베드\n",
    "# D_model(차원 수) 512 몇개 답변이 이상함\n",
    "# D_model 키우니깐 표현이 풍부해지긴했지만 질문에 맞는 답변은 아님\n",
    "\n",
    "# NUM_HEADS 변화는 성능적인 변화가 적다.\n",
    "# 다양한 문맥을 학습한다고 알고있는데 데이터셋이 간단해서 그런가? 큰 성능차이를 느낄수없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47d0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
